# cks-scenarios

---
## Question 1 
> Configure the cluster to:
>
> 1. Add **webhook mode** authorization and disable **anonymous users** in the **Kubelet**.
> 2. Enable **NodeRestriction** admission plugin and **RBAC + NodeRestriction** authorization modes in **kube-apiserver**.
> 3. Configure **etcd** with `--client-cert-auth=true`.
---
### Solutio:

#### üß± Step 1 ‚Äî Kubelet: Disable Anonymous Users & Enable Webhook Auth

#### üß± Step 1 ‚Äî Kubelet: Disable Anonymous Users & Enable Webhook Auth

1. Edit the Kubelet config file:

   ```bash
   sudo vim /var/lib/kubelet/config.yaml
   ```
2. Find or add:

   ```yaml
   authentication:
     anonymous:
       enabled: false
     webhook:
       enabled: true
   authorization:
     mode: Webhook
   ```
3. Save and restart the Kubelet:

   ```bash
   sudo systemctl restart kubelet
   ```

---

#### üß± Step 2 ‚Äî API Server: Enable RBAC + NodeRestriction

1. Edit the static pod manifest:

   ```bash
   sudo vim /etc/kubernetes/manifests/kube-apiserver.yaml
   ```

2. Inside the `command:` section, ensure these flags are present:

   ```yaml
   - --authorization-mode=NodeRestriction,RBAC
   - --enable-admission-plugins=NodeRestriction
   ```

   (If `--authorization-mode` already has something like `RBAC`, just append `,NodeRestriction`.)

3. Save and exit ‚Äî since it‚Äôs a static pod, the API server will automatically restart.

4. Verify the flags:

   ```bash
   ps -ef | grep kube-apiserver
   ```

   or check:

   ```bash
   kubectl get pods -n kube-system -l component=kube-apiserver
   ```

---

#### üß± Step 3 ‚Äî etcd: Enable Client Certificate Authentication

1. Edit etcd manifest:

   ```bash
   sudo vim /etc/kubernetes/manifests/etcd.yaml
   ```

2. Add or ensure the flag:

   ```yaml
   - --client-cert-auth=true
   ```

3. Save ‚Üí etcd pod auto-restarts.

4. Confirm:

   ```bash
   ps -ef | grep etcd
   ```

   Look for `--client-cert-auth=true`.

---

#### ‚úÖ Step 4 ‚Äî Verification

* Check Kubelet status:

  ```bash
  curl -v --insecure https://localhost:10250
  ```

  You should get an **unauthorized** response, not open access.

* Check API server admission plugins:

  ```bash
  kubectl get --raw /metrics | grep NodeRestriction
  ```
---

### üß† Summary

You hardened:

* Kubelet ‚Üí disabled anonymous access, enabled webhook authorization.
* API server ‚Üí RBAC + NodeRestriction.
* etcd ‚Üí enforces client cert authentication.

---

## Question 2

> Configure the cluster to disable **anonymous-auth** in the **kube-apiserver** and (something else ‚Äî typically enabling RBAC or ensuring proper secure port usage).

---

### Solution**

#### üß± Step 1 ‚Äî Edit API Server Static Pod

```bash
sudo vim /etc/kubernetes/manifests/kube-apiserver.yaml
```

Add or modify the following flags under `command:`:

```yaml
- --anonymous-auth=false
- --authorization-mode=RBAC,NodeRestriction
- --enable-admission-plugins=NodeRestriction
```

> üí° Note: Make sure `--authorization-mode` is not duplicated ‚Äî merge into one line if needed.

Save and exit ‚Äî the static pod will restart automatically.

---

#### üß± Step 2 ‚Äî Verify Changes

Run:

```bash
ps -ef | grep kube-apiserver
```

Ensure you see:

```
--anonymous-auth=false
--authorization-mode=RBAC,NodeRestriction
```

Check that the API server pod is running again:

```bash
kubectl get pods -n kube-system
```

---

#### üß± Step 3 ‚Äî Test Anonymous Access Blocked

Try an unauthenticated curl:

```bash
curl -k https://localhost:6443/version
```

You should see:

```
Unauthorized
```

That means anonymous-auth is disabled.

---

### üß† Summary

* API server now denies any unauthenticated access.
* RBAC controls are enforced.
* Admission plugin NodeRestriction restricts kubelet certificate usage.

---

## Question 3

> You are given:
>
> * A **Dockerfile**
> * A **Pod/Deployment YAML file**
>
> You must apply **best security practices**, including using:
>
> * A **non-root user** (`nobody`, UID `63356` provided)
> * A **readOnlyRootFilesystem** in the Pod spec
>
> Modify both files accordingly.

---

### Solution**

# **‚úîÔ∏è Part 1 ‚Äî Fix the Dockerfile**

### üîß **Before (root user)**

Example (what they usually give):

```dockerfile
FROM ubuntu:20.04
# ... some commands
USER root
CMD ["python", "app.py"]
```

### üîß **After (non-root best practice)**

If user `nobody` already exists:

```dockerfile
FROM ubuntu:20.04

# (Optional) create a custom user if UID given:
RUN useradd -u 63356 -s /sbin/nologin appuser

# Switch to non-root user
USER 63356

CMD ["python", "app.py"]
```

Or if they explicitly ask to use `nobody`:

```dockerfile
USER nobody
```

> üí° Using `USER` in Dockerfile is required for CKS.
> They expect **non-root image** + **non-root pod securityContext**.

---

# **‚úîÔ∏è Part 2 ‚Äî Fix the YAML file**

They typically give you a Deployment like:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app
spec:
  replicas: 1
  template:
    spec:
      containers:
      - name: app
        image: myimg:v1
```

### üîß **You must add `securityContext`:**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app
spec:
  replicas: 1
  template:
    spec:
      securityContext:
        runAsUser: 63356
        runAsNonRoot: true

      containers:
      - name: app
        image: myimg:v1
        securityContext:
          readOnlyRootFilesystem: true
```

---

## Question 4

> You are given a Deployment with **two containers**.
>
> You must add a **securityContext** to **both containers**, using:
>
> * `runAsUser: <ID given in question>`
> * `allowPrivilegeEscalation: false`
> * `readOnlyRootFilesystem: true`


---

### Solution**


#### **Step 1 ‚Äî Open the Deployment**

```bash
kubectl edit deployment <name>
```

or if file is provided:

```bash
vi deployment.yaml
```

---

#### **Step 2 ‚Äî Locate the two containers**

Example given:

```yaml
containers:
  - name: app1
    image: myimg1:v1
  - name: app2
    image: myimg2:v1
```

---

#### **Step 3 ‚Äî Add the required securityContext to each container**

Use the UID provided in the question.
Example: UID = `63356`

### Updated version:

```yaml
containers:
  - name: app1
    image: myimg1:v1
    securityContext:
      runAsUser: 63356
      allowPrivilegeEscalation: false
      readOnlyRootFilesystem: true

  - name: app2
    image: myimg2:v1
    securityContext:
      runAsUser: 63356
      allowPrivilegeEscalation: false
      readOnlyRootFilesystem: true
```

---

# üéØ **Final Expected YAML Answer (Ready-to-Paste)**

Replace `<UID>` with the user ID given in the exam.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: secure-app
spec:
  replicas: 1
  template:
    spec:
      containers:
      - name: app1
        image: myimg1:v1
        securityContext:
          runAsUser: <UID>
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true

      - name: app2
        image: myimg2:v1
        securityContext:
          runAsUser: <UID>
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
```

---

## Question 5

1. Create a **ServiceAccount** in namespace `monitoring`
   ‚Üí **automountServiceAccountToken: false**

2. In a **Deployment**, mount this serviceAccount‚Äôs token **as a projected volume**
   ‚Üí token must be **readOnly**
   ‚Üí must use `serviceAccountToken` type projection
   ‚Üí must specify `audience` + `path` + `expirationSeconds`
---

### Solution**
## **Step 1 ‚Äî Create the ServiceAccount**

```bash
kubectl create namespace monitoring
kubectl create serviceaccount monitor-sa -n monitoring
```

Now edit it:

```bash
kubectl edit sa monitor-sa -n monitoring
```

Add:

```yaml
automountServiceAccountToken: false
```
## **Step 2 ‚Äî Modify Deployment to use projected token**

Open file or edit:

```bash
kubectl edit deployment mydeploy -n monitoring
```

Add under `.spec.template.spec`:

### volumes:

```yaml
volumes:
- name: token-vol
  projected:
    sources:
    - serviceAccountToken:
        path: token
        audience: api
        expirationSeconds: 3600
```

### container volumeMounts:

```yaml
containers:
- name: app
  image: myimg
  volumeMounts:
  - name: token-vol
    mountPath: /var/run/secrets/tokens
    readOnly: true
```

### serviceAccountName:

```yaml
serviceAccountName: monitor-sa
```

# **FULL EXPECTED ANSWER (Copy/Paste Ready)**

### **ServiceAccount**

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: monitor-sa
  namespace: monitoring
automountServiceAccountToken: false
```

### **Deployment**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: secure-app
  namespace: monitoring
spec:
  replicas: 1
  template:
    spec:
      serviceAccountName: monitor-sa

      volumes:
      - name: token-vol
        projected:
          sources:
          - serviceAccountToken:
              path: token
              audience: api
              expirationSeconds: 3600

      containers:
      - name: app
        image: nginx
        volumeMounts:
        - name: token-vol
          mountPath: /var/run/secrets/tokens
          readOnly: true
```

---

## Question 6

1. Create a NetworkPolicy that **DENIES all ingress** traffic in namespace `prod`.
2. Create another policy that **allows traffic from pods in namespace prod ‚Üí pods in namespace data**
   Using **pod labels** + **namespace selectors**.
---

### Solution**

# üß± PART 1 ‚Äî Deny all ingress in namespace `prod`

Namespace must have **default deny** policy.

### **deny-all-ingress.yaml**

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all-ingress
  namespace: prod
spec:
  podSelector: {}     # all pods
  policyTypes:
  - Ingress
  ingress: []          # deny everything
```

# üß± PART 2 ‚Äî Allow pods from `prod` ‚Üí `data` namespace (ingress allow)

You must:

* Select pods in namespace `data` using **podSelector**
* Allow traffic only from pods in namespace `prod`
  ‚Üí using **namespaceSelector** with labels
  ‚Üí AND with podSelector

### **Pre-req:** Add labels to namespaces

```bash
kubectl label namespace prod name=prod --overwrite
kubectl label namespace data name=data --overwrite
```


### **allow-prod-to-data.yaml**

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-prod-to-data
  namespace: data
spec:
  podSelector:
    matchLabels:
      role: backend   # example: depends on question
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: prod
      podSelector:
        matchLabels:
          app: myapp   # example label from the question
```
---

# üìå FINAL FORMULA SUMMARY FOR QUESTION 6

### Deny all ingress:

```yaml
podSelector: {}
ingress: []
policyTypes: [Ingress]
```

### Allow traffic from namespace prod:

```yaml
from:
- namespaceSelector: { label: prod }
  podSelector: { label: <prod-app-label> }
```

---

## Question 7

1. You are *given* a certificate and a key, usually at paths like:

   * `/root/certs/tls.crt`
   * `/root/certs/tls.key`

2. You must create a **TLS Secret** in Kubernetes.

3. You must update a **Deployment** to use that secret, either as:
   ‚úîÔ∏è a **volume** (mounted cert/key files)
   or
   ‚úîÔ∏è environment variable (less common ‚Äî usually NOT for TLS)
---

### Solution**


## **Step 1 ‚Äî Create the TLS secret**

### Command:

```bash
kubectl create secret tls my-tls-secret \
  --cert=/root/certs/tls.crt \
  --key=/root/certs/tls.key \
  -n default
```

If they specify a namespace, use it:

```bash
kubectl create secret tls my-tls-secret \
  --cert=/root/certs/tls.crt \
  --key=/root/certs/tls.key \
  -n mynamespace
```

---

## **Step 2 ‚Äî Edit Deployment to mount the TLS secret**

Open deployment:

```bash
kubectl edit deployment mydeploy -n default
```

Under `.spec.template.spec.volumes` add:

```yaml
volumes:
- name: tls-vol
  secret:
    secretName: my-tls-secret
```

Under container `.volumeMounts` add:

```yaml
volumeMounts:
- name: tls-vol
  mountPath: /etc/tls            # choose any secure path
  readOnly: true
```

---

# üìå **FINAL YAML ANSWER (Exactly what CKS expects)**

### **TLS Secret**

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-tls-secret
  namespace: default
type: kubernetes.io/tls
data:
  tls.crt: <base64-cert>
  tls.key: <base64-key>
```

(You do **NOT** need to base64 encode if you create via `kubectl create secret tls`.)

---

### **Deployment**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydeploy
  namespace: default
spec:
  replicas: 1
  template:
    spec:
      volumes:
      - name: tls-vol
        secret:
          secretName: my-tls-secret

      containers:
      - name: app
        image: nginx
        volumeMounts:
        - name: tls-vol
          mountPath: /etc/tls
          readOnly: true
```

---

## Question 8

A namespace named **`confidential`** has **Pod Security Admission (restricted)** applied.

Meaning:

üîí Only **restricted** security settings are allowed in this namespace.
‚ùå Default pods will **NOT start** unless they meet restricted rules.

**Your task:**

üëâ Modify the Pod/Deployment YAML so it **complies with the restricted PSA**.
---

### Solution**


## **Step 1 ‚Äî Inspect namespace**

```bash
kubectl get ns confidential --show-labels
```

You will see something like:

```
pod-security.kubernetes.io/enforce=restricted
```

---

## **Step 2 ‚Äî Try to create a pod (it fails)**

You might see errors like:

```
pod is forbidden: violates PodSecurity "restricted"
```

This is expected.

---

## **Step 3 ‚Äî Patch your Pod/Deployment to satisfy "restricted"**

Add the following under each **container securityContext**:

```yaml
securityContext:
  runAsNonRoot: true
  runAsUser: 1000         # or UID provided in the exam
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: true
  capabilities:
    drop:
    - ALL
```

If the Deployment used `securityContext` at pod level, add:

```yaml
securityContext:
  fsGroup: 2000
  runAsNonRoot: true
```

(Only if needed.)

---

# üìå **FINAL YAML FIX (What the CKS grader expects)**

**Example Deployment inside `confidential` namespace:**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: secure-app
  namespace: confidential
spec:
  replicas: 1
  template:
    spec:
      containers:
      - name: app
        image: nginx
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
```

---

## Question 9

1. **Enable audit logging** in the kube-apiserver.
2. Add `--audit-policy-file=/etc/kubernetes/audit/audit.yaml`
3. Configure log rotation:

   * `--audit-log-maxage=<given>`
   * `--audit-log-maxbackup=<given>`
   * `--audit-log-maxsize=<given>` (usually also needed)
4. Create an **audit policy** with:

   * **All namespaces:** request+response metadata
   * **`web-apps` namespace:** deployments ‚Üí audit **metadata level**
   * **All namespaces:** delete on ConfigMaps & Secrets ‚Üí **RequestResponse body**
   * Everything else ‚Üí **Metadata**

---

### Solution**


# ‚úÖ **Step 1 ‚Äì Edit kube-apiserver manifest**

File:

```
/etc/kubernetes/manifests/kube-apiserver.yaml
```

Add these flags under `command:`:

```yaml
    - --audit-policy-file=/etc/kubernetes/audit/audit.yaml
    - --audit-log-path=/var/log/kubernetes/audit.log
    - --audit-log-maxage=5
    - --audit-log-maxbackup=10
    - --audit-log-maxsize=100
```

(Create directory if required)

```bash
sudo mkdir -p /etc/kubernetes/audit
```

---

# ‚úÖ **Step 2 ‚Äì Create audit policy**

File:

```
/etc/kubernetes/audit/audit.yaml
```

Content:

```yaml
apiVersion: audit.k8s.io/v1
kind: Policy

# Default level
rules:

  # 1Ô∏è‚É£ Log delete operations of ConfigMaps & Secrets with RequestResponse body
  - level: RequestResponse
    verbs: ["delete"]
    resources:
    - group: ""
      resources: ["configmaps", "secrets"]

  # 2Ô∏è‚É£ Capture Deployments interactions in web-apps namespace (metadata)
  - level: Metadata
    resources:
    - group: "apps"
      resources: ["deployments"]
    namespaces: ["web-apps"]

  # 3Ô∏è‚É£ Log ALL namespace interactions (metadata)
  - level: Metadata
    resources:
    - group: ""
      resources: ["namespaces"]

  # 4Ô∏è‚É£ Everything else at Metadata level
  - level: Metadata
```

---

# üîÑ **Step 3 ‚Äì kubelet automatically restarts kube-apiserver**

No need for manual restart because it‚Äôs a static pod under `/etc/kubernetes/manifests`.

Check:

```bash
kubectl logs -n kube-system kube-apiserver-<node>
```

Audit logs stored at:

```bash
/var/log/kubernetes/audit.log
```
---

## Question 10

1. Remove user **developer** from **docker** group
2. Disable **TCP** access to Docker daemon
3. Make Docker listen **only** on the Unix socket:

   ```
   /var/run/docker.sock
   ```
---

### Solution**
# ‚úÖ **Step 1 ‚Äî Remove user developer from docker group**

```bash
sudo gpasswd -d developer docker
```

Or:

```bash
sudo deluser developer docker
```

Confirm:

```bash
id developer
```

You should NOT see `docker` group.

---

# ‚úÖ **Step 2 ‚Äî Deny TCP traffic to Docker daemon**

Docker listens on TCP only if you explicitly enabled it in:

```
/etc/docker/daemon.json
```

Edit it to **remove any TCP host**:

### ‚ùå If you have:

```json
{
  "hosts": [
    "tcp://0.0.0.0:2375",
    "unix:///var/run/docker.sock"
  ]
}
```

### ‚úÖ Replace with:

```json
{
  "hosts": [
    "unix:///var/run/docker.sock"
  ]
}
```

---

# ‚úÖ **Step 3 ‚Äî Enforce Unix Socket Only (Mandatory CKS)**

Ensure Docker is **not listening on TCP**.

Check service file:

```
/lib/systemd/system/docker.service
```

Look for:

```
ExecStart=/usr/bin/dockerd
```

If you see any `-H tcp://...`, delete it.

Example:

### ‚ùå If you have:

```
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock
```

### ‚úÖ Replace with:

```
ExecStart=/usr/bin/dockerd -H unix:///var/run/docker.sock
```

Then reload & restart:

```bash
sudo systemctl daemon-reload
sudo systemctl restart docker
```

---

# üîê **Step 4 ‚Äî (Optional but expected in CKS) Block port 2375 using firewall**

If TCP was previously exposed, block it:

### UFW:

```bash
sudo ufw deny 2375/tcp
```

### IPTables:

```bash
sudo iptables -A INPUT -p tcp --dport 2375 -j DROP
```

---

# üîç Verification

### Check Docker isn't listening on TCP:

```bash
sudo ss -ltnp | grep dockerd
```

You should see:

```
unix  /var/run/docker.sock
```

No `tcp` entries.

---

## Question 11

upgrade nodes in cluster to v1.31.1
---

### Solution**

https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/

---

## Question 12

1. Create a TLS secret using a given key + cert file
2. Create an Ingress that uses that TLS secret
3. Add a host (given in the exam)
4. Test using curl
---

### Solution**
# ‚úÖ Step 1 ‚Äî Create TLS Secret (using provided `cert.crt` & `cert.key`)

```bash
kubectl create secret tls tls-secret \
  --cert=/path/to/cert.crt \
  --key=/path/to/cert.key \
  -n your-namespace
```

If namespace is not specified in the question ‚Üí use `default`.

---

# ‚úÖ Step 2 ‚Äî Create an Ingress using TLS

Example host from exam: `app.example.com`

```yaml
# ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: myapp-ingress
  namespace: your-namespace
spec:
  tls:
    - hosts:
        - app.example.com
      secretName: tls-secret
  rules:
    - host: app.example.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: myapp-svc
                port:
                  number: 80
```

Apply:

```bash
kubectl apply -f ingress.yaml
```

---

# ‚úÖ Step 3 ‚Äî Test Using curl (IMPORTANT FOR CKS)

If self-signed: use `-k`:

```bash
curl -k https://app.example.com
```

If DNS not resolvable, use `/etc/hosts`:

```bash
sudo sh -c 'echo "$(kubectl get ing myapp-ingress -n your-namespace -o jsonpath={.status.loadBalancer.ingress[0].ip}) app.example.com" >> /etc/hosts'
```

Then:

```bash
curl -k https://app.example.com
```

---

## Question 13

1. Enable `ImagePolicyWebhook` in kube-apiserver
2. Add the full path of `kube.conf` in `/etc/kubernetes/imagepolicy/imagepolicyfile.yaml`
3. Change default allow = **false**
4. In `kube.conf`, set the `server:` URL (given in exam)
5. Test by applying the ReplicationController YAML

---

kubeadm clusters store the kube-apiserver flags in:

```
/etc/kubernetes/manifests/kube-apiserver.yaml
```

You edit this file ‚Üí kubelet automatically restarts the API server.

---

# ‚úÖ Step 1 ‚Äî Create directory for ImagePolicyWebhook

```bash
sudo mkdir -p /etc/kubernetes/imagepolicy
```

---

# ‚úÖ Step 2 ‚Äî Create the image policy config file

File path:

```
/etc/kubernetes/imagepolicy/imagepolicyfile.yaml
```

Content:

```yaml
apiVersion: imagepolicy.k8s.io/v1alpha1
kind: ImagePolicyWebhook
allowTTL: 60
denyTTL: 60
retryBackoff: 500
defaultAllow: false
kubeConfigFile: "/etc/kubernetes/imagepolicy/kube.conf"
```

This is **the exact format used in CKS**.

---

# ‚úÖ Step 3 ‚Äî Create the kube.conf for the webhook client

Location:

```
/etc/kubernetes/imagepolicy/kube.conf
```

Content (example URL from exam):

```yaml
apiVersion: v1
kind: Config
clusters:
- cluster:
    server: https://image-webhook.example.com:443
    insecure-skip-tls-verify: true
  name: image-policy-cluster
contexts:
- context:
    cluster: image-policy-cluster
    user: image-bot
  name: image-policy-context
current-context: image-policy-context
users:
- name: image-bot
  user:
    token: dummy-token
```

The important part is:

```
server: <URL from question>
```

---

# ‚úÖ Step 4 ‚Äî Enable ImagePolicyWebhook in kube-api server

Edit:

```
sudo vi /etc/kubernetes/manifests/kube-apiserver.yaml
```

Add to command arguments:

```yaml
- --admission-control-config-file=/etc/kubernetes/imagepolicy/imagepolicyfile.yaml
- --enable-admission-plugins=ImagePolicyWebhook
```

If the line already exists, append `,ImagePolicyWebhook` to it.

Save the file ‚Äî **kubelet automatically restarts kube-apiserver**.

Check API server is running:

```bash
kubectl get pods -n kube-system
```

---

# ‚úÖ Step 5 ‚Äî Test using the provided ReplicationController YAML

The exam will give you something like:

```yaml
kubectl apply -f test-rc.yaml
```

The expected behavior:

* If the image is ALLOWED ‚Üí RC is created
* If the image is DENIED ‚Üí RC creation fails

Run:

```bash
kubectl apply -f test-rc.yaml
```

Check:

```bash
kubectl describe rc test-rc
```

Or check API server logs:

```bash
sudo journalctl -u kubelet -f
```
---

## Question 14


You have *three Deployments* of Ollama.
You must determine **which pod writes to `/dev/mem`**.
All have:

```
SecurityContext: privileged=true
```

You must identify the offending pod and mention **Falco** as the recommended tool.

---

### Solution**
---

# ‚úÖ **Step 1 ‚Äî Check security context of the 3 Deployments**

(Quick visual verification)

```bash
kubectl get deploy -n <ns>
kubectl describe deploy <name> -n <ns> | grep -i privileged -A3
```

This tells you which Deployments have:

```yaml
securityContext:
  privileged: true
```

But this **does NOT tell you which one writes to /dev/mem**.

---

# ‚úÖ **Step 2 ‚Äî Use Falco to detect writes to `/dev/mem`**

Falco has an official rule detecting this behavior:

---

## üìò **Falco Documentation Reference (important for CKS)**

Falco rule: **‚ÄúWrite below /dev‚Äù** or **"Write to special files like /dev/mem"**

Rule file locations:

* `/etc/falco/falco_rules.yaml`
* `/etc/falco/rules.d/`

Rule example:

```yaml
- rule: Write to dev mem
  desc: Detect any write attempt to /dev/mem
  condition: evt.type=write and fd.name=/dev/mem
  output: "Write to /dev/mem detected (user=%user.name pod=%k8s.pod.name container=%container.name)"
  priority: CRITICAL
```

---

# ‚úÖ **Step 3 ‚Äî Run Falco and identify the pod**

Start Falco:

```bash
sudo falco -o json_output=true
```

(or use the DaemonSet if already installed)

Watch output:

```bash
sudo journalctl -u falco -f
```

Expected alert:

```
Write to /dev/mem detected (pod=ollama-deploy-2-xxx container=ollama)
```

This gives you:

* Pod name
* Deployment name
* Container name

---

# ‚úÖ **Step 4 ‚Äî Answer (CKS-style)**

**The pod that wrote to `/dev/mem` is the one Falco reported in the alert stream.**

Falco is the **correct Kubernetes runtime security tool** to detect writes to kernel memory devices such as `/dev/mem`.

---

## Question 15
You have a Pod with **3 containers**:

```
alpine:3.14
alpine:3.16
alpine:3.18
```

You need to:

1. Scan all three images
2. Find which image contains **libproc_ar version 1.3-45**
3. Generate a BOM
4. Redirect the output to `spdx.json`

You must use: **bom** (Syft-style tool)

---

### Solution**


Credit: Anchore (open-source) ‚Äî used for SBOM generation.

Installed name varies, either:

```
syft
```

or:

```
bom
```

Exam commonly uses:

```
bom
```

---

# ‚úÖ **Step 1 ‚Äî Pull all Alpine images**

If not already pulled:

```bash
docker pull alpine:3.14
docker pull alpine:3.16
docker pull alpine:3.18
```

---

# ‚úÖ **Step 2 ‚Äî Scan each image for the library**

Run:

```bash
bom alpine:3.14 | grep -i libproc_ar
bom alpine:3.16 | grep -i libproc_ar
bom alpine:3.18 | grep -i libproc_ar
```

You search for version **1.3-45**.

Example of expected match:

```
libproc_ar 1.3-45  apk
```

---

# üü© **Result:**

The image that shows:

```
libproc_ar 1.3-45
```

is the one required by the question.

(Usually Alpine 3.14 or 3.16 contains this version, depending on exam version.)

---

# ‚úÖ **Step 3 ‚Äî Generate SPDX SBOM for that image**

Example: if the vulnerable image is `alpine:3.16`

```bash
bom alpine:3.16 -o spdx-json > spdx.json
```

Or using syft syntax:

```bash
syft alpine:3.16 -o spdx-json > spdx.json
```

---

# üéØ **CKS exam expects the EXACT output redirection line:**

```
> spdx.json
```

---